# Papers
### NLP
- [ ] [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) [[code](https://github.com/openai/finetune-transformer-lm)] [[blog](https://blog.openai.com/language-unsupervised/)]
- [ ] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
- [ ] [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/pdf/1704.04368) [[code](https://github.com/codertimo/BERT-pytorch)]
- [ ] [Semi-supervised Sequence Learning](https://arxiv.org/pdf/1511.01432)
- [ ] [Pointer Networks](https://arxiv.org/pdf/1506.03134)
- [ ] [QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/pdf/1804.09541)
- [ ] [Universal Transformers](https://arxiv.org/pdf/1807.03819)
- [ ] [Dual Learning for Machine Translation](https://arxiv.org/pdf/1611.00179)
- [ ] [A Survey of the Usages of Deep Learning in Natural Language Processing](https://arxiv.org/pdf/1807.10854)
- [ ] [Language Generation with Recurrent Generative Adversarial Networks without Pre-training](https://arxiv.org/pdf/1706.01399)
- [ ] [Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122)
- [ ] [Transformer XL](https://openreview.net/forum?id=HJePno0cYm)
- [ ] [MaskGAN: Better Text Generation via Filling in the______](https://arxiv.org/pdf/1801.07736)
- [ ] [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/pdf/1510.00726)
- [ ] [Text Understanding from Scratch](https://arxiv.org/pdf/1502.01710)

### GAN
- [ ] Spectral Normalization for Generative Adversarial Networks
- [ ] A Note on the Inception Score
- [ ] Self-Attention Generative Adversarial Networks
- [ ] Wasserstein Auto-Encoders
- [ ] [Wasserstein GAN](https://arxiv.org/abs/1701.07875)
- [ ] Improved Techniques for Training GANs
- [ ] Conditional Generative Adversarial Nets
- [ ] SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient

### Optimization
- [ ] Switchable Normalization
- [ ] Layer Normalization
- [ ] Instance Normalization
- [ ] Group Normalization
- [ ] Weight Normalization
- [ ] On the convergence of adam and beyond
- [ ] An overview of gradient descent optimization algorithms∗

### Other Important Ones (RL/Vision etc)
- [ ] Taskonomy: Disentangling Task Transfer 
- [ ] Squeeze and Excitation Networks
- [ ] Pixel Recurrent Neural Networks
- [ ] WaveNet: A Generative Model for Raw Audio
- [ ] [Parallel WaveNet: Fast High-Fidelity Speech Synthesis](https://arxiv.org/abs/1711.10433)
- [ ] [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)
- [ ] [Tacotron series](https://google.github.io/tacotron/index.html)
- [ ] [Deep Voice: Real-time Neural Text-to-Speech](https://arxiv.org/abs/1702.07825)
- [ ] [Deep Voice 2: Multi-Speaker Neural Text-to-Speech](https://arxiv.org/abs/1705.08947)
- [ ] [Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning](https://arxiv.org/abs/1710.07654)
- [ ] [Neural Voice Cloning with a Few Samples](https://arxiv.org/abs/1802.06006)
- [ ] Dynamic Routing Between Capsules
- [ ] Deep Learning Techniques for Music Generation – A Survey
- [ ] Understanding deep learning requires rethinking generalization
- [ ] Neural Turing Machines
- [ ] Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation 
- [ ] Opening the black box of deep learning
- [ ] Everybody Dance Now
- [ ] Mark-RNN
- [ ] YOLO





