# Papers
### NLP [[Tasks](https://github.com/Kyubyong/nlp_tasks)][[Preprocessing](https://towardsdatascience.com/pre-processing-in-natural-language-machine-learning-898a84b8bd47)]
- [ ] [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) [[code](https://github.com/openai/finetune-transformer-lm)] [[blog](https://blog.openai.com/language-unsupervised/)]
- [ ] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
- [ ] [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/pdf/1704.04368) [[code](https://github.com/codertimo/BERT-pytorch)]
- [ ] [Semi-supervised Sequence Learning](https://arxiv.org/pdf/1511.01432)
- [ ] [Pointer Networks](https://arxiv.org/pdf/1506.03134)
- [ ] [QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/pdf/1804.09541)
- [ ] [Universal Transformers](https://arxiv.org/pdf/1807.03819)
- [ ] [Dual Learning for Machine Translation](https://arxiv.org/pdf/1611.00179)
- [ ] [A Survey of the Usages of Deep Learning in Natural Language Processing](https://arxiv.org/pdf/1807.10854)
- [ ] [Language Generation with Recurrent Generative Adversarial Networks without Pre-training](https://arxiv.org/pdf/1706.01399)
- [ ] [Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122)
- [ ] [Transformer-XL: Language Modeling with Longer-Term Dependency](https://openreview.net/forum?id=HJePno0cYm)
- [ ] [MaskGAN: Better Text Generation via Filling in the______](https://arxiv.org/pdf/1801.07736)
- [ ] [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/pdf/1510.00726)
- [ ] [Text Understanding from Scratch](https://arxiv.org/pdf/1502.01710)
- [ ] [ELMo: Deep contextualized word representations](https://arxiv.org/pdf/1802.05365) [[allennlp](https://allennlp.org/elmo)]
- [ ] [Adaptive Input Representations for Neural Language Modeling](https://openreview.net/pdf?id=ByxZX20qFQ)
- [ ] [Trellis Networks for Sequence Modeling](https://arxiv.org/pdf/1810.06682.pdf)
- [ ] [Toward Controlled Generation of Text](https://arxiv.org/pdf/1703.00955.pdf)
- [ ] [Unsupervised Natural Language Generation with Denoising Autoencoders](https://arxiv.org/pdf/1804.07899.pdf)
- [ ] [Adversarial Generation of Natural Language](http://www.aclweb.org/anthology/W/W17/W17-2629.pdf)
- [ ] [Deep Reinforcement Learning for Dialogue Generation](https://arxiv.org/pdf/1606.01541)
- [ ] [A Unified Architecture for Natural Language Processing](https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf) [classic]
- [ ] [Reasoning about Entailment with Neural Attention](https://arxiv.org/pdf/1509.06664)
- [ ] [Learning End-to-End Goal-Oriented Dialog](https://arxiv.org/pdf/1605.07683)
- [ ] [Neural Approaches to Conversational AI](https://arxiv.org/pdf/1809.08267)
- [ ] [Machine Learning for Dialog State Tracking: A Review](https://ai.google/research/pubs/pub44018.pdf)
- [ ] [Neural Machine Translation of Rare Words with Subword Units
](http://www.aclweb.org/anthology/P16-1162)
- [ ] [Improving Neural Machine Translation Models with Monolingual Data](http://www.aclweb.org/anthology/P16-1009)
- [ ] [On Tree-Based Neural Sentence Modeling](https://arxiv.org/pdf/1808.09644)
- [ ] [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087)
- [ ] Language as a latent variable: Discrete generative models for sentence compression

### GAN
- [ ] [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/pdf/1802.05957)
- [ ] [A Note on the Inception Score](https://arxiv.org/pdf/1801.01973.pdf)
- [ ] [Self-Attention Generative Adversarial Networks](https://arxiv.org/pdf/1805.08318) [SAGAN]
- [ ] [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/pdf/1609.05473) [SeqGAN]
- [ ] [Wasserstein GAN](https://arxiv.org/abs/1701.07875) [WGAN]
- [ ] XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings
- [X] Multimodal Unsupervised Image-to-Image Translation (MUNIT)
- [ ] [The relativistic discriminator: a key element missing from standard GAN](https://arxiv.org/pdf/1807.00734) [[blog1](https://ajolicoeur.wordpress.com/relativisticgan/)][[blog2](https://medium.com/@jonathan_hui/gan-rsgan-ragan-a-new-generation-of-cost-function-84c5374d3c6e)][[code](https://github.com/AlexiaJM/RelativisticGAN)][Relativistic GANs]
- [X] [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/pdf/1606.03657) [InfoGAN]
- [ ] [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784)
- [X] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN)
- [ ] [Autoencoding beyond pixels using a learned similarity metric](https://arxiv.org/pdf/1512.09300) [VAEGAN]
- [X] Auto-Encoding Variational Bayes (VAE)
- [X] Unsupervised Image-to-Image Translation Networks (UNIT)
- [ ] [BEGAN: Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/pdf/1703.10717) [BEGAN]
- [ ] Diverse Image-to-Image Translation via Disentangled Representations (MUNIT's Bro)
- [ ] [Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data](https://arxiv.org/pdf/1802.10151) [MUNIT's Bro]
- [ ] [Toward Multimodal Image-to-Image Translation](https://arxiv.org/pdf/1711.11586) [[code](https://github.com/junyanz/BicycleGAN)] [BicyleGAN]
- [ ] Recycle-GAN: Unsupervised Video Retargeting
- [X] Adversarial Autoencoders
- [ ] [Wasserstein Auto-Encoders](https://openreview.net/pdf?id=HkL7n1-0b)
- [ ] [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/pdf/1809.11096)
- [ ] [Local Image-to-Image Translation via Pixel-wise Highway Adaptive Instance Normalization](https://openreview.net/pdf?id=HJgTHnActQ)
- [ ] [Semantically Decomposing the Latent Spaces of Generative Adversarial Networks](https://arxiv.org/pdf/1705.07904)
- [ ] [Bayesian GAN](https://arxiv.org/pdf/1705.09558)
- [ ] [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/pdf/1710.10196) [[code](https://github.com/tkarras/progressive_growing_of_gans)]
- [ ] Adversarially Learned Inference
- [ ] Improved Variational Inference with Inverse Autoregressive Flow
- [ ] CyCADA: Cycle-Consistent Adversarial Domain Adaptation
- [ ] Disentangling by Factorising
- [ ] Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks
- [ ] Generating a Fusion Image: One’s Identity and Another’s Shape
- [ ] Glow: Generative Flow with Invertible 1×1 Convolutions
- [ ] Are GANs Created Equal? A Large-Scale Study
- [ ] [Adversarially Regularized Autoencoders](https://arxiv.org/pdf/1706.04223)
- [ ] [Generalized Denoising Auto-Encoders as Generative Models](http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models.pdf)
- [ ] Adversarial examples for generative models
- [ ] GAN Q-learning
- [ ] AdaGAN: Boosting Generative Models
- [ ] Unrolled Generative Adversarial Networks
- [ ] Do GANs actually learn the distribution? An empirical study
- [X] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation
- [ ] CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms
- [ ] [Variational Inference: A Review for Statisticians](https://arxiv.org/pdf/1601.00670)
- [ ] [Variational Inference with Normalizing Flows](https://arxiv.org/pdf/1505.05770)

#### GAN Training
- [ ] Towards Principled Methods for Training Generative Adversarial Networks
- [ ] Improved Training of Wasserstein GANs
- [ ] [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498)

### Optimization
- [ ] Switchable Normalization
- [ ] Layer Normalization
- [ ] Instance Normalization
- [ ] Group Normalization
- [ ] Weight Normalization
- [ ] On the convergence of adam and beyond
- [ ] An overview of gradient descent optimization algorithms∗

### Other Important Ones (RL/Vision etc)
- [ ] Taskonomy: Disentangling Task Transfer
- [ ] Squeeze and Excitation Networks
- [ ] Pixel Recurrent Neural Networks
- [ ] WaveNet: A Generative Model for Raw Audio
- [ ] [Parallel WaveNet: Fast High-Fidelity Speech Synthesis](https://arxiv.org/abs/1711.10433)
- [ ] [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)
- [ ] [Tacotron series](https://google.github.io/tacotron/index.html)
- [ ] [Deep Voice: Real-time Neural Text-to-Speech](https://arxiv.org/abs/1702.07825)
- [ ] [Deep Voice 2: Multi-Speaker Neural Text-to-Speech](https://arxiv.org/abs/1705.08947)
- [ ] [Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning](https://arxiv.org/abs/1710.07654)
- [ ] [Neural Voice Cloning with a Few Samples](https://arxiv.org/abs/1802.06006)
- [ ] Dynamic Routing Between Capsules
- [ ] Deep Learning Techniques for Music Generation – A Survey
- [ ] Understanding deep learning requires rethinking generalization
- [ ] Neural Turing Machines
- [ ] Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation
- [ ] Opening the black box of deep learning
- [ ] Everybody Dance Now
- [ ] [Mask R-CNN](https://arxiv.org/pdf/1703.06870) [[blog](https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272)]


### Blog Articles
- [ ] [Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)
- [ ] Object Recognition: [[part1](https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html)][[part2](https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html)][[part3](https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html)]
- [ ] Reinforcement Learning:
	- [The Multi-Armed Bandit Problem and Its Solutions](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)
	- [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
	- [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)
	- [Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym](https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html)
- [ ] Generative Models:
	- [Flow-based Deep Generative Models ](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html)
	- [From Autoencoder to Beta-VAE](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)
-
